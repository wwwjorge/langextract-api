# =============================================================================
# CONFIGURAÇÃO PARA DEPLOY NO PORTAINER - LANGEXTRACT API
# =============================================================================
# Copie este arquivo para o Portainer como variáveis de ambiente da stack

# -----------------------------------------------------------------------------
# PORTAS DE ACESSO
# -----------------------------------------------------------------------------
# Porta principal da API LangExtract (expor via Cloudflare Tunnel)
LANGEXTRACT_API_PORT=8000

# Porta do Ollama (opcional, para modelos locais)
OLLAMA_PORT=11434

# -----------------------------------------------------------------------------
# CHAVES DE API DOS PROVEDORES LLM
# -----------------------------------------------------------------------------
# Chave da API Gemini (OBRIGATÓRIA para usar Gemini)
GEMINI_API_KEY=your_gemini_api_key_here

# Chave da API OpenAI (opcional)
OPENAI_API_KEY=your_openai_api_key_here

# Chave da API LangExtract (opcional, se usar provedor específico)
LANGEXTRACT_API_KEY=your_langextract_api_key_here

# -----------------------------------------------------------------------------
# CONFIGURAÇÕES DE SEGURANÇA DA API
# -----------------------------------------------------------------------------
# Chave secreta para JWT (GERE UMA CHAVE FORTE!)
API_SECRET_KEY=your_super_secret_jwt_key_change_this_in_production_min_32_chars

# Algoritmo de criptografia JWT
API_ALGORITHM=HS256

# Tempo de expiração do token em minutos
API_ACCESS_TOKEN_EXPIRE_MINUTES=60

# -----------------------------------------------------------------------------
# CONFIGURAÇÕES DO MODELO PADRÃO
# -----------------------------------------------------------------------------
# Modelo padrão a ser usado
DEFAULT_MODEL=gemini-1.5-flash

# Provedor padrão (gemini, openai, ollama)
DEFAULT_PROVIDER=gemini

# -----------------------------------------------------------------------------
# CONFIGURAÇÕES DE UPLOAD E PROCESSAMENTO
# -----------------------------------------------------------------------------
# Tamanho máximo de arquivo em MB
MAX_FILE_SIZE_MB=100

# Número máximo de workers para processamento paralelo
MAX_WORKERS=4

# -----------------------------------------------------------------------------
# CONFIGURAÇÕES DE REDE E CORS
# -----------------------------------------------------------------------------
# Origens permitidas para CORS (* permite todas)
CORS_ORIGINS=*

# URL base do Ollama (se usar modelos locais)
OLLAMA_BASE_URL=http://ollama:11434

# Configurações do Ollama
OLLAMA_ORIGINS=*
OLLAMA_HOST=0.0.0.0

# -----------------------------------------------------------------------------
# CONFIGURAÇÕES DE LOG
# -----------------------------------------------------------------------------
# Nível de log (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# =============================================================================
# INSTRUÇÕES DE USO NO PORTAINER:
# =============================================================================
# 1. Copie todo o conteúdo deste arquivo
# 2. No Portainer, vá em Stacks > Add Stack
# 3. Cole o conteúdo do docker-compose.portainer.yml
# 4. Na seção "Environment variables", cole as variáveis acima
# 5. Ajuste os valores conforme necessário (principalmente as API keys)
# 6. Deploy da stack
# =============================================================================

# IMPORTANTE: 
# - Substitua 'your_gemini_api_key_here' pela sua chave real do Gemini
# - Gere uma chave secreta forte para API_SECRET_KEY
# - Ajuste as portas se necessário para evitar conflitos
# - Configure o Cloudflare Tunnel para a porta definida em LANGEXTRACT_API_PORT