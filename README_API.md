# LangExtract FastAPI

API REST para extra√ß√£o estruturada de dados de texto usando LLMs (Large Language Models).

## üöÄ Caracter√≠sticas

- **Autentica√ß√£o JWT**: Sistema de autentica√ß√£o seguro
- **M√∫ltiplos Provedores**: Suporte para OpenAI, Google Gemini e Ollama
- **Processamento de Arquivos**: Suporte para TXT, MD, JSON (PDF e DOCX em desenvolvimento)
- **CORS Configur√°vel**: Configura√ß√£o flex√≠vel para diferentes origens
- **Documenta√ß√£o Autom√°tica**: Swagger UI e ReDoc integrados
- **Modelo Padr√£o**: Gemini-2.5-Flash como padr√£o

## üìã Pr√©-requisitos

- Python 3.8+
- Chaves de API configuradas no arquivo `.env`
- Ollama instalado (opcional, para modelos locais)

## üõ†Ô∏è Instala√ß√£o

1. **Instalar depend√™ncias:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configurar vari√°veis de ambiente:**
   - Copie as chaves de API do arquivo `.env` principal para `api/.env`
   - Ajuste as configura√ß√µes conforme necess√°rio

3. **Iniciar a API:**
   ```bash
   python start_api.py
   ```

## üåê Endpoints

### Autentica√ß√£o
- `POST /auth/token` - Obter token JWT
  - Usu√°rio padr√£o: `admin` / `admin`

### Extra√ß√£o
- `POST /extract` - Extrair dados de texto
- `POST /extract/file` - Extrair dados de arquivo

### Informa√ß√µes
- `GET /health` - Status da API e modelos dispon√≠veis
- `GET /models` - Lista de modelos por provedor
- `GET /providers` - URLs dos provedores
- `GET /docs` - Documenta√ß√£o Swagger
- `GET /redoc` - Documenta√ß√£o ReDoc

## üìù Exemplo de Uso

### 1. Obter Token de Autentica√ß√£o
```bash
curl -X POST "http://localhost:8001/auth/token" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=admin&password=admin"
```

### 2. Extrair Dados de Texto
```bash
curl -X POST "http://localhost:8001/extract" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Maria estava feliz. Jo√£o estudava medicina.",
    "prompt": "Extract people and their emotions",
    "model_id": "gemini-2.5-flash",
    "temperature": 0.3
  }'
```

### 3. Extrair de Arquivo
```bash
curl -X POST "http://localhost:8001/extract/file" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "file=@documento.txt" \
  -F "prompt=Extract key information" \
  -F "model_id=gemini-2.5-flash"
```

## üîß Configura√ß√£o

### Arquivo `api/.env`
```env
# API Settings
API_SECRET_KEY=your-super-secret-jwt-key
API_HOST=0.0.0.0
API_PORT=8001

# Default Model
DEFAULT_MODEL=gemini-2.5-flash
DEFAULT_TEMPERATURE=0.3

# CORS
CORS_ORIGINS=*
ALLOWED_METHODS=GET,POST,PUT,DELETE,OPTIONS

# File Upload
MAX_FILE_SIZE_MB=100
ALLOWED_FILE_EXTENSIONS=.txt,.pdf,.docx,.md,.json

# Provider URLs
OLLAMA_BASE_URL=http://localhost:11434
```

## ü§ñ Modelos Suportados

### OpenAI
- gpt-4o
- gpt-4o-mini
- gpt-4-turbo
- gpt-3.5-turbo

### Google Gemini
- gemini-2.5-flash (padr√£o)
- gemini-1.5-pro
- gemini-1.5-flash

### Ollama (Local)
- Qualquer modelo instalado localmente
- Detec√ß√£o autom√°tica via API do Ollama

## üìä Estrutura de Resposta

```json
{
  "success": true,
  "extractions": [
    {
      "class": "person",
      "text": "Maria",
      "attributes": {
        "emotion": "happy"
      }
    }
  ],
  "model_used": "gemini-2.5-flash",
  "provider": "gemini",
  "processing_time": 1.23,
  "error": null
}
```

## üß™ Teste

Execute o script de teste:
```bash
python test_api.py
```

O teste verifica:
- ‚úÖ Health check
- ‚úÖ Autentica√ß√£o
- ‚úÖ Listagem de modelos
- ‚úÖ Listagem de provedores
- ‚úÖ Extra√ß√£o de texto

## üê≥ Deploy com Docker

### Usando Docker Compose

```bash
# Clone o reposit√≥rio
git clone <repository-url>
cd langextract-api

# Configure as vari√°veis de ambiente
cp .env.sample .env
# Edite o .env com suas configura√ß√µes

# Execute com Docker Compose
docker-compose up -d
```

### Deploy no Portainer

1. **Via GitHub Registry:**
   - Use o arquivo `portainer-stack.yml`
   - Configure as vari√°veis de ambiente no Portainer
   - Deploy como Stack

2. **Build Local:**
   ```bash
   docker build -t langextract-api .
   docker run -d -p 8001:8001 --env-file .env langextract-api
   ```

### GitHub Actions

O projeto inclui workflow automatizado que:
- Faz build da imagem Docker
- Executa testes automatizados
- Publica no GitHub Container Registry
- Executa scan de seguran√ßa

### Configura√ß√£o de Produ√ß√£o

**Vari√°veis Essenciais:**
```env
API_SECRET_KEY=your-production-secret-key
API_ACCESS_TOKEN=your-production-api-token
CORS_ORIGINS=https://yourdomain.com
OPENAI_API_KEY=your-openai-key
GEMINI_API_KEY=your-gemini-key
CLOUDFLARE_API_TOKEN=your-cloudflare-token
```

## üöÄ Pr√≥ximos Passos

- [x] Containeriza√ß√£o com Docker
- [x] Deploy automatizado via GitHub Actions
- [x] Suporte a Cloudflare Workers AI
- [x] Configura√ß√£o de timeouts
- [ ] Monitoramento avan√ßado
- [ ] Cache de respostas
- [ ] Rate limiting por usu√°rio

## üîí Seguran√ßa

- Autentica√ß√£o JWT obrigat√≥ria
- Valida√ß√£o de tipos de arquivo
- Limite de tamanho de upload
- Headers de seguran√ßa configur√°veis
- N√£o exposi√ß√£o de chaves de API

## üìà Monitoramento

- Endpoint `/health` para verifica√ß√£o de status
- Logs detalhados via Uvicorn
- Tempo de processamento nas respostas
- Detec√ß√£o autom√°tica de modelos dispon√≠veis

## üö® Troubleshooting

### Erro de Autentica√ß√£o
- Verifique se o token JWT est√° v√°lido
- Confirme usu√°rio/senha (padr√£o: admin/admin)

### Modelo N√£o Encontrado
- Verifique se as chaves de API est√£o configuradas
- Para Ollama, confirme se o servi√ßo est√° rodando

### Erro de CORS
- Ajuste `CORS_ORIGINS` no arquivo `.env`
- Verifique se os headers est√£o corretos

## üìû URLs Importantes

- **API Base**: http://localhost:8001
- **Documenta√ß√£o**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc
- **Health Check**: http://localhost:8001/health