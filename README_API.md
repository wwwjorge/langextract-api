# LangExtract API

Uma API REST baseada em FastAPI para o framework LangExtract do Google, fornecendo acesso program√°tico √†s funcionalidades de extra√ß√£o de informa√ß√µes estruturadas de textos usando modelos de linguagem.

## üöÄ Caracter√≠sticas

- **API REST completa** com documenta√ß√£o autom√°tica (Swagger/OpenAPI)
- **Autentica√ß√£o JWT** configur√°vel
- **Suporte a m√∫ltiplos provedores LLM** (Gemini, OpenAI, Ollama)
- **Upload de arquivos** (TXT, MD, JSON, PDF, DOCX)
- **Processamento ass√≠ncrono** com status de jobs
- **Visualiza√ß√£o HTML** dos resultados
- **Containeriza√ß√£o Docker** completa
- **Configura√ß√£o flex√≠vel** via vari√°veis de ambiente
- **Logs estruturados** para monitoramento

## üìã Pr√©-requisitos

- Python 3.8+
- Docker e Docker Compose (recomendado)
- Chaves de API para provedores LLM (opcional)

## üõ† Instala√ß√£o R√°pida

### 1. Clone e Configure

```bash
# Clone o reposit√≥rio
git clone https://github.com/google/langextract
cd langextract

# Execute o script de setup
python setup_api.py
```

### 2. Configure as Vari√°veis de Ambiente

Edite o arquivo `.env` criado:

```env
# Configura√ß√£o da API
API_SECRET_KEY=seu-secret-key-muito-seguro
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Chaves dos Provedores LLM
GEMINI_API_KEY=sua-chave-gemini
OPENAI_API_KEY=sua-chave-openai
LANGEXTRACT_API_KEY=sua-chave-langextract

# Configura√ß√£o Ollama
OLLAMA_HOST=http://ollama:11434
OLLAMA_BASE_URL=http://ollama:11434

# Modelo padr√£o
DEFAULT_MODEL_ID=gemma2:2b
DEFAULT_PROVIDER=ollama
```

### 3. Inicie os Servi√ßos

```bash
# Com Docker (recomendado)
docker-compose up -d

# Ou com Python diretamente
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

## üìö Documenta√ß√£o da API

Ap√≥s iniciar os servi√ßos, acesse:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

## üîê Autentica√ß√£o

### Obter Token de Acesso

```bash
curl -X POST http://localhost:8000/auth/token \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin"}'
```

Resposta:
```json
{
  "access_token": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...",
  "token_type": "bearer",
  "expires_in": 1800
}
```

### Usar Token nas Requisi√ß√µes

```bash
curl -H "Authorization: Bearer SEU_TOKEN" \
  http://localhost:8000/models
```

## üéØ Exemplos de Uso

### 1. Listar Modelos Dispon√≠veis

```bash
curl -H "Authorization: Bearer SEU_TOKEN" \
  http://localhost:8000/models
```

### 2. Extra√ß√£o de Texto Simples

```bash
curl -X POST http://localhost:8000/extract/text \
  -H "Authorization: Bearer SEU_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Jo√£o Silva trabalha como engenheiro na empresa XYZ desde 2020.",
    "prompt_description": "Extrair informa√ß√µes sobre pessoas",
    "examples": [
      {
        "input": "Maria Santos √© m√©dica no Hospital ABC.",
        "output": {"nome": "Maria Santos", "profissao": "m√©dica", "local": "Hospital ABC"}
      }
    ],
    "model_id": "gemma2:2b",
    "temperature": 0.1
  }'
```

### 3. Upload e Processamento de Arquivo

```bash
# Upload do arquivo
curl -X POST http://localhost:8000/extract/file \
  -H "Authorization: Bearer SEU_TOKEN" \
  -F "file=@documento.pdf" \
  -F "prompt_description=Extrair informa√ß√µes de contato" \
  -F "model_id=gemma2:2b"
```

### 4. Verificar Status da Extra√ß√£o

```bash
curl -H "Authorization: Bearer SEU_TOKEN" \
  http://localhost:8000/extract/status/EXTRACTION_ID
```

### 5. Baixar Resultados

```bash
# Resultado JSON
curl -H "Authorization: Bearer SEU_TOKEN" \
  http://localhost:8000/extract/download/EXTRACTION_ID/json

# Visualiza√ß√£o HTML
curl -H "Authorization: Bearer SEU_TOKEN" \
  http://localhost:8000/extract/download/EXTRACTION_ID/html
```

## üîß Configura√ß√£o Avan√ßada

### Par√¢metros de Extra√ß√£o

A API suporta todos os par√¢metros do LangExtract:

```json
{
  "text": "Texto para processar",
  "prompt_description": "Descri√ß√£o da tarefa",
  "examples": [{"input": "exemplo", "output": {"resultado": "esperado"}}],
  "model_id": "gemma2:2b",
  "temperature": 0.1,
  "max_char_buffer": 10000,
  "max_workers": 4,
  "batch_length": 1000,
  "extraction_passes": 1,
  "format_type": "json",
  "additional_context": "Contexto adicional",
  "debug": false
}
```

### Configura√ß√£o de Modelos

#### Gemini
```json
{
  "model_id": "gemini-2.0-flash-exp",
  "api_key": "sua-chave-gemini",
  "temperature": 0.1
}
```

#### OpenAI
```json
{
  "model_id": "gpt-4",
  "api_key": "sua-chave-openai",
  "temperature": 0.1
}
```

#### Ollama (Local)
```json
{
  "model_id": "gemma2:2b",
  "model_url": "http://localhost:11434",
  "temperature": 0.1
}
```

## üê≥ Docker

### Servi√ßos Dispon√≠veis

- **langextract-api**: API principal (porta 8000)
- **ollama**: Servidor Ollama local (porta 11434)

### Comandos √öteis

```bash
# Iniciar servi√ßos
docker-compose up -d

# Ver logs
docker-compose logs -f langextract-api
docker-compose logs -f ollama

# Parar servi√ßos
docker-compose down

# Rebuild
docker-compose up --build

# Executar comando na API
docker-compose exec langextract-api python -c "import langextract; print('OK')"
```

### Volumes

- `uploads/`: Arquivos enviados
- `outputs/`: Resultados das extra√ß√µes
- `logs/`: Logs da aplica√ß√£o
- `ollama_data/`: Dados do Ollama

## üìä Monitoramento

### Health Check

```bash
curl http://localhost:8000/health
```

Resposta:
```json
{
  "status": "healthy",
  "timestamp": "2024-01-15T10:30:00Z",
  "version": "1.0.0",
  "services": {
    "langextract": "available",
    "ollama": "available"
  }
}
```

### Logs Estruturados

Os logs s√£o estruturados em JSON para facilitar an√°lise:

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "info",
  "event": "extraction_started",
  "extraction_id": "ext_123",
  "model_id": "gemma2:2b",
  "text_length": 1500
}
```

## üîí Seguran√ßa

### Produ√ß√£o

1. **Altere senhas padr√£o**:
   ```env
   API_SECRET_KEY=chave-muito-segura-e-aleatoria
   ```

2. **Configure CORS**:
   ```env
   CORS_ORIGINS=https://seu-dominio.com,https://app.seu-dominio.com
   ```

3. **Use HTTPS**:
   - Configure proxy reverso (nginx, traefik)
   - Certificados SSL/TLS

4. **Limite rate limiting**:
   ```env
   RATE_LIMIT_REQUESTS=100
   RATE_LIMIT_WINDOW=3600
   ```

## üö® Troubleshooting

### Problemas Comuns

#### 1. Erro de Autentica√ß√£o
```bash
# Verifique se o token est√° v√°lido
curl -H "Authorization: Bearer SEU_TOKEN" http://localhost:8000/auth/verify
```

#### 2. Modelo N√£o Encontrado
```bash
# Liste modelos dispon√≠veis
curl -H "Authorization: Bearer SEU_TOKEN" http://localhost:8000/models
```

#### 3. Ollama N√£o Conecta
```bash
# Verifique se o Ollama est√° rodando
docker-compose logs ollama

# Teste conex√£o direta
curl http://localhost:11434/api/tags
```

#### 4. Erro de Upload
```bash
# Verifique tamanho do arquivo (limite: 50MB)
# Verifique formato suportado: .txt, .md, .json, .pdf, .docx
```

### Logs de Debug

Para debug detalhado, configure:

```env
LOG_LEVEL=DEBUG
DEBUG=true
```

## üìà Performance

### Recomenda√ß√µes

1. **Para textos longos**:
   - Use `max_char_buffer` apropriado
   - Configure `max_workers` baseado no CPU
   - Use `batch_length` para dividir processamento

2. **Para alta concorr√™ncia**:
   - Aumente workers do uvicorn
   - Configure pool de conex√µes
   - Use cache Redis (opcional)

3. **Modelos locais (Ollama)**:
   - Requer GPU para melhor performance
   - Configure mem√≥ria adequada
   - Use modelos menores para testes

## ü§ù Contribui√ß√£o

1. Fork o reposit√≥rio
2. Crie branch para feature (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudan√ßas (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para branch (`git push origin feature/nova-funcionalidade`)
5. Abra Pull Request

## üìÑ Licen√ßa

Este projeto segue a mesma licen√ßa do LangExtract original.

## üÜò Suporte

- **Issues**: https://github.com/google/langextract/issues
- **Documenta√ß√£o**: http://localhost:8000/docs
- **Logs**: `docker-compose logs -f langextract-api`

---

**Desenvolvido com ‚ù§Ô∏è usando FastAPI e LangExtract**